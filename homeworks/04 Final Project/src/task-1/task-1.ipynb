{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92c19c15",
   "metadata": {},
   "source": [
    "# Final Project - Task 1: Barebone MLP\n",
    "In this task, you will build a Multi-Layer Perceptron (MLP) from first principles by implementing both the forward and backward passes yourselves.\n",
    "\n",
    "## Instructions:\n",
    "- Each numbered `# TODO-k` marks a line of code that you are required to complete. Make sure you replace the `...` with your code. \n",
    "    - It's recommended to start each `TODO` by reviewing the relevant material and deriving the equation with good old pen and paper. Only then move on to the implementation.\n",
    "- After some of the `TODO`s, there are check cells. You only need to run these cells, and no new code is required. These cells will verify that your implementation is correct. If your code is correct, the check cell should run without errors.\n",
    "- Do not modify any other parts of the code. Only fill in the sections marked with `# TODO`.\n",
    "- You are only allowed to use the `NumPy` library in your answers. Do not use PyTorch, TensorFlow, scikit-learn, or any other libraries.\n",
    "    - Some checks and the last part use PyTorch and require no code to be written\n",
    "\n",
    "- Once all TODOs are completed:\n",
    "    - run ALL cells (even pre-written ones)\n",
    "    - The model should successfully train on a synthetic regression task.\n",
    "    - The final Mean Squared Error (MSE) should be less than 1e-3.\n",
    "\n",
    "- Submit your completed notebook along with answers to the theoretical questions provided in the project report.\n",
    "\n",
    "## Additional Notes:\n",
    "- Read the explanations before each section carefully.\n",
    "- Pay close attention to what each function receives and returns. Shape annotations are provided to guide your implementation.\n",
    "- For those who want, a Linear algebra (e.g. what is a gradient) refresher is available [here](https://stanford.edu/~shervine/teaching/cs-229/refresher-algebra-calculus). Consulting with LLMs is advised as well!\n",
    "\n",
    "By completing this task, you will have implemented:\n",
    "- Linear layers (forward and backward)\n",
    "- ReLU activation functions (forward and backward)\n",
    "- Mean Squared Error loss (forward and backward)\n",
    "- A simple training loop using stochastic gradient descent (SGD)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17d024f",
   "metadata": {},
   "source": [
    "# A. Multilayer Perceptron (MLP)\n",
    "\n",
    "An **MLP** (also known as feedforward) is a chain of linear transformations and activation functions applied to an input vector. This class of functions serves as a potent hypothesis class. As such, multilayered perceptrons form the basis of deep learning and are applicable across a vast set of diverse domains.\n",
    "\n",
    "\n",
    "## One Input, One Hidden Layer\n",
    "\n",
    "Suppose the input is a scalar $x \\in \\mathbb{R}$, and we want to compute 3 hidden features ($h_1$, $h_2$, $h_3$). Each feature is computed using:\n",
    "\n",
    "$$\n",
    "h_i = a(\\theta_{i0} + \\theta_{i1} \\cdot x)\n",
    "$$\n",
    "\n",
    "For example, all three:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "h_1 &= a(\\theta_{10} + \\theta_{11} x) \\\\\n",
    "h_2 &= a(\\theta_{20} + \\theta_{21} x) \\\\\n",
    "h_3 &= a(\\theta_{30} + \\theta_{31} x)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "This can be written more compactly using vectors and matrices:\n",
    "\n",
    "$$\n",
    "\\mathbf{h} = a\\left(\n",
    "\\begin{bmatrix}\n",
    "\\theta_{10} \\\\\n",
    "\\theta_{20} \\\\\n",
    "\\theta_{30}\n",
    "\\end{bmatrix}\n",
    "+\n",
    "\\begin{bmatrix}\n",
    "\\theta_{11} \\\\\n",
    "\\theta_{21} \\\\\n",
    "\\theta_{31}\n",
    "\\end{bmatrix}\n",
    "x\n",
    "\\right)\n",
    "= a(\\boldsymbol{\\theta}_0 + \\boldsymbol{\\theta} \\cdot x)\n",
    "$$\n",
    "\n",
    "\n",
    "## Multiple featuers\n",
    "\n",
    "If $\\mathbf{x} \\in \\mathbb{R}^3$ (e.g., $x = [x_1, x_2, x_3]$), and we want 3 hidden units, each one is:\n",
    "\n",
    "$$\n",
    "h_i = a(\\theta_{i0} + \\theta_{i1} x_1 + \\theta_{i2} x_2 + \\theta_{i3} x_3)\n",
    "$$\n",
    "\n",
    "Written as a matrix:\n",
    "\n",
    "$$\n",
    "\\mathbf{h} = a\\left(\n",
    "\\begin{bmatrix}\n",
    "\\theta_{10} \\\\\n",
    "\\theta_{20} \\\\\n",
    "\\theta_{30}\n",
    "\\end{bmatrix}\n",
    "+\n",
    "\\begin{bmatrix}\n",
    "\\theta_{11} & \\theta_{12} & \\theta_{13} \\\\\n",
    "\\theta_{21} & \\theta_{22} & \\theta_{23} \\\\\n",
    "\\theta_{31} & \\theta_{32} & \\theta_{33}\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "x_1 \\\\\n",
    "x_2 \\\\\n",
    "x_3\n",
    "\\end{bmatrix}\n",
    "\\right)\n",
    "= a(\\boldsymbol{\\theta}_0 + \\boldsymbol{\\Theta} \\mathbf{x})\n",
    "$$\n",
    "\n",
    "\n",
    "## Multiple Samples (Batch Input)\n",
    "Now we have:\n",
    "* A Single input vector $\\mathbf{x} \\in \\mathbb{R}^3$\n",
    "* A batch $X \\in \\mathbb{R}^{B \\times 3}$, where each row is an input sample\n",
    "Suppose we have a **batch** of $B$ input vectors, each of dimension 3. We stack them into a matrix:\n",
    "\n",
    "$$\n",
    "X =\n",
    "\\begin{bmatrix}\n",
    "x_1^{(1)} & x_2^{(1)} & x_3^{(1)} \\\\\n",
    "x_1^{(2)} & x_2^{(2)} & x_3^{(2)} \\\\\n",
    "\\vdots    & \\vdots    & \\vdots    \\\\\n",
    "x_1^{(B)} & x_2^{(B)} & x_3^{(B)}\n",
    "\\end{bmatrix}\n",
    "\\in \\mathbb{R}^{B \\times 3}\n",
    "$$\n",
    "\n",
    "Now, we can calclaute the 3 hidden units in parallel using matrix multiplication. This means, we calculate the values in each layer for all samples at the same time, and this is why our output now has multiple rows. The parameters are:\n",
    "\n",
    "* Weight matrix $\\boldsymbol{\\Theta} \\in \\mathbb{R}^{3 \\times 3}$\n",
    "* Bias vector $\\boldsymbol{\\theta}_0 \\in \\mathbb{R}^{3}$\n",
    "\n",
    "In matrix form, for all $B$ samples:\n",
    "\n",
    "$$\n",
    "H =\n",
    "a\\left(\n",
    "\\begin{bmatrix}\n",
    "x_1^{(1)} & x_2^{(1)} & x_3^{(1)} \\\\\n",
    "x_1^{(2)} & x_2^{(2)} & x_3^{(2)}\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "\\theta_{11} & \\theta_{21} & \\theta_{31} \\\\\n",
    "\\theta_{12} & \\theta_{22} & \\theta_{32} \\\\\n",
    "\\theta_{13} & \\theta_{23} & \\theta_{33}\n",
    "\\end{bmatrix}\n",
    "+\n",
    "\\begin{bmatrix}\n",
    "\\theta_{10} & \\theta_{20} & \\theta_{30}\n",
    "\\end{bmatrix}\n",
    "\\right)\n",
    "\\in \\mathbb{R}^{2 \\times 3}\n",
    "$$\n",
    "\n",
    "So:\n",
    "\n",
    "* Each row of $H$ is a hidden representation for one sample\n",
    "* Each column of $\\boldsymbol{\\Theta}$ defines weights for one input feature across all hidden units\n",
    "* Bias is broadcast across the batch\n",
    "\n",
    "\n",
    "## Final Formulation\n",
    "\n",
    "If we denote:\n",
    "\n",
    "* $X \\in \\mathbb{R}^{B \\times \\text{in\\_dim}}$\n",
    "* $W = \\boldsymbol{\\Theta} \\in \\mathbb{R}^{\\text{out\\_dim} \\times \\text{in\\_dim}}$\n",
    "* $b = \\boldsymbol{\\theta}_0 \\in \\mathbb{R}^{\\text{out\\_dim}}$\n",
    "\n",
    "Then the full batched layer is:\n",
    "\n",
    "$$\n",
    "Z = X W^\\top + b \\quad \\in \\mathbb{R}^{B \\times \\text{out\\_dim}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "H = a(Z)\n",
    "$$\n",
    "\n",
    "This is exactly the form used in modern deep learning frameworks: batched input matrix, weight matrix, bias vector, and optional activation.\n",
    "\n",
    "\n",
    "## Next Layers Use Same Pattern\n",
    "\n",
    "The output of the first layer, $\\mathbf{h}$, becomes the input to the next:\n",
    "\n",
    "$$\n",
    "\\mathbf{h}' = a(\\boldsymbol{\\psi}_0 + \\boldsymbol{\\Psi} \\cdot \\mathbf{h})\n",
    "$$\n",
    "\n",
    "And so on.\n",
    "\n",
    "## Final Output Layer (No Activation)\n",
    "\n",
    "The last layer usually skips the activation (especially in regression):\n",
    "\n",
    "$$\n",
    "\\mathbf{y} = \\boldsymbol{\\phi}^\\top \\cdot \\mathbf{h}' + \\phi_0\n",
    "$$\n",
    "\n",
    "## Summary (General Form)\n",
    "\n",
    "Each layer applies the same operation:\n",
    "\n",
    "$$\n",
    "\\mathbf{h}^{(l)} = a\\left(\\mathbf{h}^{(l-1)} \\mathbf{W}^{(l)\\top} + \\mathbf{b}^{(l)\\top}\\right)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "* $\\mathbf{W}^{(l)}$ is a matrix of weights\n",
    "* $\\mathbf{b}^{(l)}$ is a vector of biases\n",
    "* $a$ is the activation function (e.g., ReLU)\n",
    "\n",
    "# B. Terminology: Forward and Backward Passes\n",
    "\n",
    "**Forward Pass**  \n",
    "The *forward pass* refers to the sequence of computations that occur from the input layer to the output layer of a neural network. During this phase, the model generates predictions (`y_hat`) based on current weights and biases.\n",
    "\n",
    "At each layer:\n",
    "- Inputs are transformed using the layer’s parameters.\n",
    "- The output of one layer becomes the input to the next.\n",
    "\n",
    "For example, a linear layer followed by ReLU:\n",
    "- $z = xW^\\top + b$\n",
    "- $a = \\text{ReLU}(z)$\n",
    "\n",
    "The forward pass is also used to compute the loss, which quantifies how far the predictions are from the true labels.\n",
    "\n",
    "**Backward Pass (Backpropagation)**  \n",
    "The *backward pass* is the process of computing gradients of the loss function with respect to the model’s parameters. This is done using the chain rule of calculus, applied from the output layer back through the network.\n",
    "\n",
    "At each layer:\n",
    "- The gradient of the loss is propagated backward.\n",
    "- Gradients with respect to weights, biases, and inputs are computed.\n",
    "\n",
    "These gradients are then used to update the parameters using an optimization algorithm such as stochastic gradient descent (SGD).\n",
    "\n",
    "For example:\n",
    "- Given $\\frac{\\partial \\text{Loss}}{\\partial a}$, compute:\n",
    "  - $\\frac{\\partial \\text{Loss}}{\\partial W}$\n",
    "  - $\\frac{\\partial \\text{Loss}}{\\partial b}$\n",
    "  - $\\frac{\\partial \\text{Loss}}{\\partial x}$\n",
    "\n",
    "The backward pass is essential for learning, as it tells the model how to adjust its parameters to reduce the loss."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e7e6c8b",
   "metadata": {},
   "source": [
    "## 1  Linear layer\n",
    "### 1.1 Linear Forward `TODO-1`\n",
    "In the function `linear_forward` fill in the missing code to calculate `z`, the pre-activation output of the a single layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8c540d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def linear_forward(x, params):\n",
    "    \"\"\"\n",
    "    Forward pass through a linear (fully connected) layer.\n",
    "\n",
    "    Parameters:\n",
    "        x (ndarray): Input of shape (batch_size, input_dim)\n",
    "        params (dict): Contains:\n",
    "            - \"W\": Weights of shape (output_dim, input_dim)\n",
    "            - \"b\": Biases of shape (output_dim,)\n",
    "\n",
    "    Returns:\n",
    "        z (ndarray): Output of shape (batch_size, output_dim)\n",
    "        cache (tuple): (x, params), cached for backward pass\n",
    "    \"\"\"\n",
    "    W, b = params[\"W\"], params[\"b\"]\n",
    "    \n",
    "    # TODO-1: implement forward linear pass\n",
    "    # === Your code here ===\n",
    "    z = ...\n",
    "    # === Your code end ===\n",
    "\n",
    "    cache = (x, params)\n",
    "    return z, cache\n",
    "\n",
    "\n",
    "def initialize_linear_layer(input_dim, output_dim):\n",
    "    \"\"\"\n",
    "    Initialize weights and biases for a linear (fully connected) layer using He initialization.\n",
    "\n",
    "    Returns a dictionary containing:\n",
    "        - \"W\": Weight matrix (output_dim, input_dim)\n",
    "        - \"b\": Bias vector (output_dim,)\n",
    "        - \"dW\": Gradient of weights (same shape as W), initialized to zeros\n",
    "        - \"db\": Gradient of biases (same shape as b), initialized to zeros\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"W\": np.random.randn(output_dim, input_dim) * (2 / input_dim) ** 0.5,\n",
    "        \"b\": np.zeros(output_dim),\n",
    "        \"dW\": np.zeros((output_dim, input_dim)),\n",
    "        \"db\": np.zeros(output_dim),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12bafadb",
   "metadata": {},
   "source": [
    "### Check Cell\n",
    "Run the cell below to see if your linear_forward is correctly implemented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b170719a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Check for TODO-1: linear_forward ===\n",
    "#\n",
    "#  Tests your implementation using a 2x3 input and a 2x3 weight matrix.\n",
    "\n",
    "# Input matrix: shape (batch_size=2, input_dim=3)\n",
    "x_test = np.array([\n",
    "    [1.0, 2.0, 3.0],\n",
    "    [0.5, -1.0, 2.0]\n",
    "])\n",
    "\n",
    "# Parameters:\n",
    "# W has shape (output_dim=2, input_dim=3)\n",
    "# b has shape (output_dim=2,)\n",
    "params_test = {\n",
    "    \"W\": np.array([\n",
    "        [0.1, 0.2, 0.3],\n",
    "        [-0.5, 0.0, 0.5]\n",
    "    ]),\n",
    "    \"b\": np.array([0.1, -0.2])\n",
    "}\n",
    "\n",
    "# Resulting shape: (2, 2)\n",
    "expected_z = np.array([\n",
    "    [1.5, 0.8],\n",
    "    [0.55, 0.55]\n",
    "])\n",
    "\n",
    "# Run student code\n",
    "z_out, _ = linear_forward(x_test, params_test)\n",
    "\n",
    "# Check\n",
    "assert np.allclose(z_out, expected_z), f\"Incorrect result.\\nGot:\\n{z_out}\\nExpected:\\n{expected_z}\"\n",
    "print(\"✅ linear_forward passed the 2x3 input test.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5db0c5e",
   "metadata": {},
   "source": [
    "### 1·2 Linear backward `TODO-2`\n",
    "In the backward pass of a linear (fully connected) layer, you are given the gradient of the loss with respect to the layer's output $\\partial L / \\partial z$, and are required to calculate the gradient of the loss with respect to the model paramters and inputs: $\\partial L / \\partial x$, $\\partial L / \\partial W$, $\\partial L / \\partial b$ using the chain rule.\n",
    "\n",
    "![bpmlp.png](bpmlp.png)\n",
    "\n",
    "| Symbol      | Meaning                                             | Shape                                      |\n",
    "|-------------|------------------------------------------------------|---------------------------------------------|\n",
    "| `x`         | Input to the layer                                   | $(B,\\ \\text{in\\_dim})$                       |\n",
    "| `W`         | Weight matrix                                        | $(\\text{out\\_dim},\\ \\text{in\\_dim})$         |\n",
    "| `b`         | Bias vector                                          | $(\\text{out\\_dim})$                          |\n",
    "| `z`         | Pre-activation output                                | $(B,\\ \\text{out\\_dim})$                      |\n",
    "| `a`         | Post-activation output                               | $(B,\\ \\text{out\\_dim})$                      |\n",
    "| `y_hat`     | Model predictions                                    | $(B,\\ \\text{out\\_dim})$                      |\n",
    "| `y`         | Ground truth targets                                 | $(B,\\ \\text{out\\_dim})$                      |\n",
    "| `dz`        | $\\partial L / \\partial z$: gradient of loss w.r.t. layer output | $(B,\\ \\text{out\\_dim})$           |\n",
    "| `dx`        | $\\partial L / \\partial x$: gradient of loss w.r.t. input        | $(B,\\ \\text{in\\_dim})$            |\n",
    "| `dW`        | $\\partial L / \\partial W$: gradient of loss w.r.t. weights      | $(\\text{out\\_dim},\\ \\text{in\\_dim})$ |\n",
    "| `db`        | $\\partial L / \\partial b$: gradient of loss w.r.t. biases       | $(\\text{out\\_dim})$               |\n",
    "\n",
    "\n",
    "**Notes:**\n",
    "- Remember that the loss is a mean \n",
    "    - $\\ell^{(i)}$: The **loss for a single sample** $i$\n",
    "    - $L$: The **total loss**, averaged over the batch: $L = \\frac{1}{B} \\sum_{i=1}^B \\ell^{(i)} = \\frac{1}{B} \\sum_{i=1}^B l(\\hat{y}_1 - y_1)$\n",
    "\n",
    "> Think of $\\ell^{(i)}$ as how \"wrong\" the network is on one example,\n",
    "> and $L$ as how wrong it is **on average** across the batch.\n",
    "\n",
    "- $B$ is the batch size (i.e., number of samples in the input $x$).\n",
    "- `in_dim` is the number of input features.\n",
    "- `out_dim` is the number of output features (e.g., neurons in the layer)\n",
    "- All gradients are computed during the **backward pass** and used to update the parameters during training.\n",
    "\n",
    "\n",
    "**Hints:**\n",
    "- Use `.T` for transposing when needed.\n",
    "- Divide `dW`,`db` by the batch size $B$ (to average the gradients)\n",
    "- Do not divide `dx` by the batch size $B$, since $x$ is not a paramter.\n",
    "- Match the expected output shapes exactly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b710954",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_backward(dz, cache):\n",
    "    \"\"\"\n",
    "    Backward pass through a linear layer.\n",
    "\n",
    "    Parameters:\n",
    "        dz (ndarray): Gradient of loss w.r.t. output z, shape (batch_size, output_dim)\n",
    "        cache (tuple): Contains:\n",
    "            - x (ndarray): Input from forward pass, shape (batch_size, input_dim)\n",
    "            - params (dict): Layer parameters\n",
    "\n",
    "    Returns:\n",
    "        dx (ndarray): Gradient of loss w.r.t. input x, shape (batch_size, input_dim)\n",
    "    \"\"\"\n",
    "    x, params = cache\n",
    "    W = params[\"W\"]\n",
    "\n",
    "    # TODO-2: compute gradients\n",
    "    # === Your code here ===\n",
    "    dW = ...\n",
    "    db = ...\n",
    "    dx = ...\n",
    "    # === Your code end ===\n",
    "\n",
    "    params[\"dW\"] = dW\n",
    "    params[\"db\"] = db    \n",
    "    return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a0e4d7",
   "metadata": {},
   "source": [
    "Check-Cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13dc86ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, numpy as np\n",
    "\n",
    "B, in_dim, out_dim = 4, 5, 3\n",
    "\n",
    "x_t = torch.randn(B, in_dim,  requires_grad=True)\n",
    "W_t = torch.randn(out_dim, in_dim, requires_grad=True)\n",
    "b_t = torch.randn(out_dim,      requires_grad=True)\n",
    "\n",
    "# Forward + backward in PyTorch (loss = SUM, not mean)\n",
    "z_t   = x_t @ W_t.T + b_t\n",
    "loss  = z_t.sum()                # ∂L/∂z = 1\n",
    "loss.backward()\n",
    "\n",
    "# Build NumPy copies\n",
    "x   = x_t.detach().numpy()\n",
    "W   = W_t.detach().numpy()\n",
    "b   = b_t.detach().numpy()\n",
    "dz  = np.ones_like(z_t.detach().numpy())    # upstream gradient = 1  (matches loss = sum)\n",
    "\n",
    "params = {\"W\": W.copy(),\n",
    "          \"b\": b.copy(),\n",
    "          \"dW\": np.zeros_like(W),\n",
    "          \"db\": np.zeros_like(b)}\n",
    "cache  = (x, params)\n",
    "\n",
    "\n",
    "# Compare \n",
    "dx = linear_backward(dz, cache)   # uses /B inside\n",
    "np.testing.assert_allclose(params[\"dW\"], W_t.grad.numpy() / B, rtol=1e-5)\n",
    "np.testing.assert_allclose(params[\"db\"], b_t.grad.numpy() / B, rtol=1e-5)\n",
    "np.testing.assert_allclose(dx,            x_t.grad.numpy(),     rtol=1e-5)\n",
    "\n",
    "print(\"✅ linear_backward passes autograd test with your current implementation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed8a14d",
   "metadata": {},
   "source": [
    "## 2 ReLU Activation\n",
    "\n",
    "After the linear transformation, we apply a nonlinearity.\n",
    "In this exercise, we use the **ReLU (Rectified Linear Unit)** function:\n",
    "\n",
    "$$\n",
    "\\text{ReLU}(z) = \\max(0,\\ z)\n",
    "$$\n",
    "\n",
    "This function is applied **element-wise**, meaning:\n",
    "\n",
    "* Each individual number is replaced by itself if it’s positive,\n",
    "* Or by 0 if it’s negative.\n",
    "\n",
    "### Example\n",
    "\n",
    "Suppose the output of the linear layer for a batch of 2 samples is:\n",
    "\n",
    "$$\n",
    "Z =\n",
    "\\begin{bmatrix}\n",
    "-1.2 & 0.0 & 3.4 \\\\\n",
    "2.1 & -0.5 & 1.3\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Then applying ReLU element-wise gives:\n",
    "\n",
    "$$\n",
    "A = \\text{ReLU}(Z) =\n",
    "\\begin{bmatrix}\n",
    "0.0 & 0.0 & 3.4 \\\\\n",
    "2.1 & 0.0 & 1.3\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "In the backward part, we must return `da` which is: $\\partial L/\\partial a$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e6b3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_forward(z):\n",
    "    \"\"\"\n",
    "    Forward pass through a ReLU activation (element-wise max with 0).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    z : ndarray, shape (batch_size, features)\n",
    "        Pre-activation values coming from the previous linear layer\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    out   : ndarray, shape (batch_size, features)\n",
    "        Post-activation values\n",
    "    cache : ndarray\n",
    "        A copy of the input `a`, saved so the backward pass\n",
    "        can determine which entries were “active”.\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO-3: implement forward ReLU pass\n",
    "    # === Your code here ===\n",
    "    out = ...  # element-wise ReLU\n",
    "    # === Your code end ===\n",
    "\n",
    "    return out, z            # cache the raw input\n",
    "\n",
    "\n",
    "def relu_backward(dout, a):\n",
    "    \"\"\"\n",
    "    Backward pass for a ReLU layer.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dout : ndarray, shape (batch_size, features)\n",
    "        Gradient of the loss with respect to the ReLU output.\n",
    "    a    : ndarray, shape (batch_size, features)\n",
    "        Same tensor that was given as `a` to `relu_forward`\n",
    "        (cached during the forward pass).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    da : ndarray, shape (batch_size, features)\n",
    "        Gradient of the loss with respect to the ReLU input.\n",
    "    \"\"\"\n",
    "    dz = dout.copy()         # keep upstream gradient\n",
    "    # TODO-4: apply ReLU mask\n",
    "    # === Your code here ===\n",
    "    da = ...             # Hint: mask dz where a <= 0\n",
    "    # === Your code end ===\n",
    "    return da"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b1bb5e",
   "metadata": {},
   "source": [
    "Check-cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9230748a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Check for TODO-3 and TODO-4: ReLU forward / backward ===\n",
    "import numpy as np\n",
    "\n",
    "# Tiny 2×4 tensor with positive, zero, and negative entries\n",
    "z_test = np.array([[ 1.0, -2.0,  0.0, 3.0],\n",
    "                   [-1.5,  2.5, -0.3, 0.0]], dtype=float)\n",
    "\n",
    "# ---------- Forward check ----------\n",
    "out, cache = relu_forward(z_test)\n",
    "\n",
    "expected_forward = np.array([[1.0, 0.0, 0.0, 3.0],\n",
    "                             [0.0, 2.5, 0.0, 0.0]], dtype=float)\n",
    "\n",
    "np.testing.assert_array_equal(out, expected_forward)\n",
    "print(\"✅ ReLU forward passes simple mask test.\")\n",
    "\n",
    "# ---------- Backward check ----------\n",
    "# Upstream gradient: ones, same shape\n",
    "dout = np.ones_like(z_test)\n",
    "\n",
    "grad = relu_backward(dout, cache)\n",
    "\n",
    "# Should propagate 1 where z_test>0, else 0\n",
    "expected_grad = np.array([[1.0, 0.0, 0.0, 1.0],\n",
    "                          [0.0, 1.0, 0.0, 0.0]], dtype=float)\n",
    "\n",
    "np.testing.assert_array_equal(grad, expected_grad)\n",
    "print(\"✅ ReLU backward passes simple mask test.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86b9bdd",
   "metadata": {},
   "source": [
    "## 3 Loss: Mean-Squared Error (MSE)\n",
    "\n",
    "Fill in the **forward** and **backward** code blocks for the MSE loss layer.\n",
    "\n",
    "#### Forward pass\n",
    "\n",
    "*Inputs*\n",
    "\n",
    "* $y_{\\text{hat}}\\in\\mathbb{R}^{B\\times d}$ — model predictions\n",
    "* $y\\in\\mathbb{R}^{B\\times d}$ — ground-truth targets\n",
    "\n",
    "*Compute*\n",
    "\n",
    "1. The element-wise residual matrix\n",
    "\n",
    "   $$\n",
    "     z\\;=\\;y_{\\text{hat}}-y\n",
    "     \\qquad\\bigl(\\text{same shape }B\\times d\\bigr)\n",
    "   $$\n",
    "\n",
    "2. The loss value\n",
    "\n",
    "   $$\n",
    "     L = \\operatorname{MSE}(y_{\\text{hat}},y)\n",
    "   $$\n",
    "\n",
    "\n",
    "#### Backward pass\n",
    "\n",
    "Produce the gradient with respect to the predictions, i.e.\n",
    "\n",
    "$$\n",
    "  g \\;=\\;\\frac{\\partial L}{\\partial y_{\\text{hat}}}\n",
    "  \\quad\\in\\mathbb{R}^{B\\times d}.\n",
    "$$\n",
    "\n",
    "Return this tensor `g` so it can be propagated to earlier layers.\n",
    "\n",
    "\n",
    "**Notation**\n",
    "\n",
    "| quantity | meaning                              | shape       |\n",
    "| -------- | ------------------------------------ | ----------- |\n",
    "| $z$      | residuals $y_{\\text{hat}}-y$         | $B\\times d$ |\n",
    "| $L$      | MSE loss                             | scalar      |\n",
    "| $g$      | $\\partial L/\\partial y_{\\text{hat}}$ | $B\\times d$ |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e572b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_forward(y_hat, y):\n",
    "    \"\"\"\n",
    "    Mean-Squared-Error forward pass.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_hat : ndarray, shape (batch_size, out_dim)\n",
    "        Model predictions.\n",
    "    y      : ndarray, shape (batch_size, out_dim)\n",
    "        Ground-truth targets.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    loss : float\n",
    "        Scalar MSE value\n",
    "    diff : ndarray, shape (batch_size, out_dim)\n",
    "        Residuals cached for the backward step.\n",
    "    \"\"\"\n",
    "    # TODO-5: compute mean squared error loss\n",
    "    # === Your code here ===\n",
    "    diff = ...                  # (B, d)\n",
    "    loss = ...         # scalar\n",
    "    # === Your code end ===\n",
    "    return loss, diff\n",
    "\n",
    "\n",
    "def mse_backward(diff):\n",
    "    \"\"\"\n",
    "    Mean-Squared-Error backward pass.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    diff : ndarray, shape (batch_size, out_dim)\n",
    "        Residuals from mse_forward (i.e., y_hat − y).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    grad : ndarray, shape (batch_size, out_dim)\n",
    "        Gradient of the loss with respect to y_hat.\n",
    "    \"\"\"\n",
    "    batch_size = diff.shape[0]\n",
    "\n",
    "    # TODO-6: compute gradient of the loss w.r.t. y_hat\n",
    "    # === Your code here ===\n",
    "    g = ...\n",
    "    # === Your code end ===\n",
    "    return g\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d51309",
   "metadata": {},
   "source": [
    "Check-cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abff54db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tiny deterministic example\n",
    "y_hat = np.array([[3.0, 2.0],\n",
    "                  [1.0, 4.0]], dtype=float)   # (B=2, d=2)\n",
    "\n",
    "y_true = np.array([[2.0, 0.0],\n",
    "                   [1.0, 5.0]], dtype=float)\n",
    "\n",
    "loss, diff = mse_forward(y_hat, y_true)\n",
    "\n",
    "expected_diff  = np.array([[ 1.0,  2.0],\n",
    "                           [ 0.0, -1.0]], dtype=float)\n",
    "\n",
    "expected_loss  = ((expected_diff**2).mean())  \n",
    "\n",
    "np.testing.assert_array_equal(diff, expected_diff)\n",
    "np.testing.assert_allclose(loss, expected_loss, rtol=1e-12)\n",
    "print(\"✅ mse_forward produces correct residuals and loss.\")\n",
    "\n",
    "grad = mse_backward(diff)\n",
    "\n",
    "expected_grad = np.array([[ 1.0,  2.0],\n",
    "                          [ 0.0, -1.0]], dtype=float)\n",
    "\n",
    "np.testing.assert_array_equal(grad, expected_grad)\n",
    "print(\"✅ mse_backward returns correct gradient.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea811478",
   "metadata": {},
   "source": [
    "## 4  Gradient descent\n",
    "You're implementing the parameter update step in training. After computing gradients during backpropagation, this function updates the model's weights and biases\n",
    "\n",
    "For each layer:\n",
    "\n",
    "* `W` and `b` hold the current weights and biases.\n",
    "* `dW` and `db` hold their respective gradients.\n",
    "\n",
    "This step nudges the parameters in the direction that reduces the loss.\n",
    "\n",
    "The `zero_grads` function clears out the gradients **after** the step, so the next minibatch starts fresh. Without it, gradients from multiple steps would accumulate and give incorrect updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70bbe597",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd_step(param_list, lr):\n",
    "    \"\"\"\n",
    "    Perform SGD update step on all parameters.\n",
    "\n",
    "    Parameters:\n",
    "        param_list (list): List of parameter dicts with keys \"W\", \"b\", \"dW\", \"db\"\n",
    "        lr (float): Learning rate\n",
    "    \"\"\"\n",
    "    for p in param_list:\n",
    "        W = p[\"W\"]\n",
    "        dw = p[\"dW\"]\n",
    "        b = p[\"b\"]\n",
    "        db = p[\"db\"]\n",
    "\n",
    "        # TODO-7: implement SGD step\n",
    "        # === Your code here ===\n",
    "        W = ...\n",
    "        b = ...\n",
    "        # === Your code end ===\n",
    "\n",
    "        p[\"W\"] = W            # update weight\n",
    "        p[\"b\"] = b            # update bias\n",
    "\n",
    "def zero_grads(param_list):\n",
    "    \"\"\"\n",
    "    Zero out all gradients in the parameter list.\n",
    "\n",
    "    Parameters:\n",
    "        param_list (list): List of parameter dicts\n",
    "    \"\"\"\n",
    "    for params in param_list:\n",
    "        params[\"dW\"].fill(0)\n",
    "        params[\"db\"].fill(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495e28c4",
   "metadata": {},
   "source": [
    "## 5  Build the MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb00810a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_mlp(input_dim, hidden_dims, output_dim):\n",
    "    \"\"\"\n",
    "    Build a list of layers representing the MLP.\n",
    "\n",
    "    Parameters:\n",
    "        input_dim (int): Dimension of input features\n",
    "        hidden_dims (list): List of integers for hidden layer sizes\n",
    "        output_dim (int): Dimension of output\n",
    "\n",
    "    Returns:\n",
    "        layers (list): List of (layer_type, params)\n",
    "        params (list): List of parameter dicts\n",
    "    \"\"\"\n",
    "    dims = [input_dim] + hidden_dims + [output_dim]\n",
    "    layers, params = [], []\n",
    "    for i in range(len(dims) - 1):\n",
    "        p = initialize_linear_layer(dims[i], dims[i+1])\n",
    "        layers.append((\"linear\", p))\n",
    "        params.append(p)\n",
    "        if i < len(dims) - 2:\n",
    "            layers.append((\"relu\", None))\n",
    "    return layers, params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73d3d42",
   "metadata": {},
   "source": [
    "## 6  Whole-network forward & backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9b13da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp_forward(x, layers):\n",
    "    \"\"\"\n",
    "    Forward pass through the full MLP.\n",
    "\n",
    "    Parameters:\n",
    "        x (ndarray): Input of shape (batch_size, input_dim)\n",
    "        layers (list): List of (layer_type, params) tuples\n",
    "\n",
    "    Returns:\n",
    "        output (ndarray): Output of the network, shape (batch_size, output_dim)\n",
    "        caches (list): Cached intermediate values for backward pass\n",
    "    \"\"\"\n",
    "    caches = []\n",
    "    for layer_type, params in layers:\n",
    "        if layer_type == \"linear\":\n",
    "            x, cache = linear_forward(x, params)\n",
    "        else:\n",
    "            x, cache = relu_forward(x)\n",
    "        caches.append((layer_type, cache))\n",
    "    return x, caches\n",
    "\n",
    "def mlp_backward(dout, caches):\n",
    "    \"\"\"\n",
    "    Backward pass through the full MLP.\n",
    "\n",
    "    Parameters:\n",
    "        dout (ndarray): Gradient from final loss layer, shape (batch_size, output_dim)\n",
    "        caches (list): Cached forward pass values\n",
    "\n",
    "    Returns:\n",
    "        None (updates gradients in-place)\n",
    "    \"\"\"\n",
    "    for layer_type, cache in reversed(caches):\n",
    "        if layer_type == \"linear\":\n",
    "            dout = linear_backward(dout, cache)\n",
    "        else:\n",
    "            dout = relu_backward(dout, cache)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba44fc9",
   "metadata": {},
   "source": [
    "## 7  Synthetic sanity check\n",
    "In this final part, you'll train your full MLP model on a small synthetic regression task. The input data X is randomly generated, and the targets Y are computed using a known linear transformation. Your goal is to learn the weights of this transformation using a neural network. You'll start by setting hyperparameters like the learning rate, number of epochs, batch size, and hidden layer sizes. The training loop performs standard mini-batch gradient descent: it shuffles the data, runs forward and backward passes through the network, updates weights using the gradients, and prints out the training loss every few epochs. At the end, you'll check that the model has learned well by verifying that the final mean squared error is sufficiently small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2041d25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic regression data\n",
    "N, D, C = 20, 5, 3\n",
    "X = np.random.randn(N, D)\n",
    "true_W = np.random.randn(C, D)\n",
    "Y = X @ true_W.T\n",
    "\n",
    "# TODO-8: set hyperparameters\n",
    "# === Your code here ===\n",
    "lr, epochs = ...\n",
    "batch_size = ...\n",
    "hidden_dimensions = [...]  # Example hidden layer size, can be a list like [1,2,3,...]\n",
    "# === Your code end ===\n",
    "\n",
    "# Build the MLP\n",
    "layers, params = build_mlp(D, hidden_dimensions, C)\n",
    "\n",
    "for ep in range(epochs):\n",
    "    # Shuffle data\n",
    "    indices = np.random.permutation(N)\n",
    "    X_shuffled, Y_shuffled = X[indices], Y[indices]\n",
    "\n",
    "    for i in range(0, N, batch_size):\n",
    "        x_batch = X_shuffled[i:i+batch_size]\n",
    "        y_batch = Y_shuffled[i:i+batch_size]\n",
    "\n",
    "        # TODO-9: forward pass, loss computation (use previously defined functions)\n",
    "        # === Your code here ===\n",
    "        y_hat, caches = ...\n",
    "        loss, diff = ...\n",
    "        # === Your code end ===\n",
    "\n",
    "        # Backward pass\n",
    "        dL = mse_backward(diff)\n",
    "        mlp_backward(dL, caches)\n",
    "\n",
    "        sgd_step(params, lr)\n",
    "\n",
    "        # Zero out gradients for the next iteration\n",
    "        zero_grads(params)\n",
    "\n",
    "    if ep % 20 == 0:\n",
    "        print(f\"epoch {ep:03d}  loss {loss:.4e}\")\n",
    "\n",
    "# Evaluate final loss\n",
    "final = ((mlp_forward(X, layers)[0] - Y) ** 2).mean()\n",
    "print(\"final MSE:\", final)\n",
    "\n",
    "# Assert the loss is sufficiently small\n",
    "assert final < 1e-3, f\"MSE too high: {final:.4e} (expected < 1e-3)\"\n",
    "print(\"✅ Training reached MSE < 1e-3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e0a9f2",
   "metadata": {},
   "source": [
    "With every TODO filled, **final MSE < 1e-3**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d88743a",
   "metadata": {},
   "source": [
    "## 8 From PyTorch to NumPy – Understanding the Moving Parts of an MLP\n",
    "\n",
    "PyTorch is a deep learning framework that automates much of what we've done manually in NumPy.\n",
    "Now that we’ve built a full MLP (multi-layer perceptron) from scratch using only NumPy, let’s pause and appreciate what we’ve accomplished, while contrasting it with how things are done in PyTorch.\n",
    "\n",
    "NumPy gives you full control: you compute everything manually. This helps you learn the math and logic behind backpropagation.\n",
    "\n",
    "PyTorch automates almost everything, making it faster and more readable, especially for larger models or real-world tasks. Once you understand the fundamentals, using PyTorch lets you focus on higher-level ideas, such as:\n",
    "\n",
    "* Experimenting with architectures\n",
    "* Trying different optimizers or loss functions without writing complex derivatives every time\n",
    "* Scaling up to real-world problems\n",
    "\n",
    "In short: PyTorch automatically tracks computations, builds a computational graph, and performs backpropagation under the hood. You only need to define your model, loss, and optimizer, and PyTorch handles the rest.\n",
    "\n",
    "\n",
    "| Component            | PyTorch                                 | NumPy (our code)                            |\n",
    "| -------------------- | --------------------------------------- | ------------------------------------------- |\n",
    "| Layer definitions    | `torch.nn.Linear(...)`                  | `initialize_linear_layer(...)`              |\n",
    "| Activation functions | `torch.nn.ReLU()`                       | `relu_forward(...)`                         |\n",
    "| Forward pass         | `model(x)` or `Sequential.forward(...)` | `mlp_forward(...)`                          |\n",
    "| Backward gradients   | `loss.backward()`                       | `mlp_backward(...)`, `linear_backward(...)` |\n",
    "| Parameter updates    | `optimizer.step()`                      | `sgd_step(...)`                             |\n",
    "| Zeroing gradients    | `optimizer.zero_grad()`                 | `zero_grads(...)`                           |\n",
    "| Loss computation     | `torch.nn.MSELoss()`                    | `mse_forward(...)`, `mse_backward(...)`     |\n",
    "\n",
    "\n",
    "Let’s run a this example, a PyTorch version of our code (it's so much shorter!) and try to understand how to use it correctly. PyTorch will make our life easier in tasks 2 and 3 of the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e19dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Convert our existing data to PyTorch\n",
    "X_tensor = torch.from_numpy(X).float()\n",
    "Y_tensor = torch.from_numpy(Y).float()\n",
    "\n",
    "# Define the MLP \n",
    "class TorchMLP(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = torch.nn.Sequential(\n",
    "            torch.nn.Linear(5, 64),     # match input_dim = 5\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(64, 64),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(64, 3)      # match output_dim = 3\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# Create model\n",
    "model = TorchMLP()\n",
    "print(model)\n",
    "\n",
    "# Define loss and optimizer\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(1, 400):\n",
    "    y_pred = model(X_tensor)\n",
    "    loss = loss_fn(y_pred, Y_tensor)\n",
    "\n",
    "    optimizer.zero_grad() # Zero gradients\n",
    "    loss.backward() # Compute gradients\n",
    "    optimizer.step() # Update parameters (Gradient Descent)\n",
    "\n",
    "    # Print loss every 20 epochs or at the first epoch\n",
    "    if epoch % 20 == 0 or epoch == 1:\n",
    "        print(f\"epoch {epoch:03d} | loss {loss.item():.4e}\")\n",
    "\n",
    "# Final check\n",
    "final_loss = loss_fn(model(X_tensor), Y_tensor).item()\n",
    "print(f\"\\nFinal MSE: {final_loss:.4e}\")\n",
    "\n",
    "assert final_loss < 1e-3, f\"MSE too high: {final_loss:.4e}\"\n",
    "print(\"✅ PyTorch model reached MSE < 1e-3\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hw1-00660121",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
