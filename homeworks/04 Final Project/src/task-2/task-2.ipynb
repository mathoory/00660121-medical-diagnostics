{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cab983ed",
   "metadata": {},
   "source": [
    "# Task 2\n",
    "In this notebook you will benchmark a nonlinear model (MLP) against the linear regression baseline from HW1 on the same mini‑GEO gene‑expression task (predict a single target gene from landmark genes). \n",
    "\n",
    "We have provided you with skeleton code to read the dataset file, you should complete the missing code, and modify any code as you need to complete Task 2.\n",
    "\n",
    "Reuse / adapt your HW1 linear regression code to record its test MSE and Pearson $r$, and then design, train, and tune a PyTorch MLP (choose hidden layer sizes, activation, learning rate, batch size, epochs, optional dropout...). You’ll track train/validation losses to pick hyper‑parameters, and finally compare LR vs the best MLP on the held‑out test set.\n",
    "\n",
    "The TODO blocks below correspond exactly to those steps: fill them in, run the notebook end‑to‑end, and copy the resulting metrics/plots into your report. Keep runs short at first (few epochs, small model) to verify everything works before tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d071162",
   "metadata": {},
   "source": [
    "## Linear Regression (LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882ceebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = pd.read_csv('gene_expression_regression.csv')\n",
    "data_array = data.iloc[:, 1:].values  # Skip the sample ID column\n",
    "\n",
    "X = data_array[:, :943]       # First 943 genes = input\n",
    "Y = data_array[:, 943:]       # Remaining genes = targets\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "X_test, X_val, Y_test, Y_val = train_test_split(\n",
    "    X_test, Y_test, test_size=0.5, random_state=42)\n",
    "\n",
    "\n",
    "# TODO-1 reuse and adapt HW1 code to train a LR model and record its test MSE and Pearson r\n",
    "# === YOUR CODE HERE ===\n",
    "\n",
    "# == END OF YOUR CODE ==="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9bc760",
   "metadata": {},
   "source": [
    "## Multilayered Perceptron (MLP)\n",
    "\n",
    "TODO-2 – Hyper-parameters: This block is where you set all the hyperparamters that control training: batch_size (how many samples each gradient step sees) and epochs (how many full passes over the training data). You are free to add any other hyper-paramters here too: learning rate, hidden-layer widths, dropout (if using). so you can tweak them from one place.\n",
    "\n",
    "TODO-3 – Model definition: Decide on the model architechture (which layers are you using, what are their size, activation functions (e.g. nn.ReLU()), and optionally also nn.Dropout as needed). The final layer should output a single value (nn.Linear(..., 1)). See example in `task-1.ipynb`.\n",
    "\n",
    "TODO-4 – Training loop: Implement a loop that trains the model, by using the predefined optimizer (Adam is an advanced optimization algorithm that builds upon the basic ideas of Gradient Descent.) and criterion (MSE). See example in `task-1.ipynb`.\n",
    "\n",
    "Once those TODOs are filled, the evaluation block we provided to you will compute test-set MSE so you can compare models.\n",
    "\n",
    "TODO-5 - Any other plots required for the report, evaluations on the validation set and code you might need to add."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76097b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "import numpy as np\n",
    "\n",
    "# TODO-2: Define Hyperparameters \n",
    "# === YOUR CODE HERE ===\n",
    "batch_size = ...\n",
    "epochs = ...\n",
    "# (more can be added as needed)\n",
    "# == END OF YOUR CODE ===\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(Y_train, dtype=torch.float32).view(-1, 1)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(Y_test, dtype=torch.float32).view(-1, 1)\n",
    "X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(Y_val, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fda10a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            # TODO-3: Implement your model\n",
    "            # === YOUR CODE HERE ===\n",
    "\n",
    "            # == END OF YOUR CODE ===\n",
    "         ...\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "model = MLP()\n",
    "print(model)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "\n",
    "    for batch_X, batch_y in train_loader:\n",
    "            # TODO-4: Implement the training step\n",
    "            # === YOUR CODE HERE ===\n",
    "            ...\n",
    "            # == END OF YOUR CODE ==="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1049448",
   "metadata": {},
   "source": [
    "## Evalutation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def33a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO-5: Validation\n",
    "# === YOUR CODE HERE ===\n",
    "\n",
    "# == END OF YOUR CODE ==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf770d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Evaluation\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred = model(X_test_tensor).numpy().flatten()\n",
    "    y_true = y_test_tensor.numpy().flatten()\n",
    "    test_mse = np.mean((y_true - y_pred) ** 2)\n",
    "\n",
    "print(f\"Test MSE: {test_mse:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
