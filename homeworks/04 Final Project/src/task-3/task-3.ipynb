{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7874363e",
   "metadata": {},
   "source": [
    "## Model Architechture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98bdba1",
   "metadata": {},
   "source": [
    "In this section you will **implement the convolutional neural network** that scores RNA sequences (encoded as one‑hot tensors) for binding. The scaffold below contains a `GlobalCNN` class with a placeholder (`TODO-1`) where you must fill in the architecture.\n",
    "\n",
    "The function we will be modeling is:\n",
    "\n",
    " $$f(\\text{sequence})=P(\\text{binding}|\\text{sequence})$$\n",
    "\n",
    "### Your Goals\n",
    "\n",
    "1. **Understand the expected input shape**: `(N, 1, 4, L)`  \n",
    "   - `N` = batch size  \n",
    "   - First dimension after batch is a dummy channel (set to 1)  \n",
    "   - Height = 4 rows (A, C, G, U order **must** match your encoder)  \n",
    "   - Width = fixed sequence length after padding/truncation (e.g. `L = 501`)\n",
    "\n",
    "2. **Design the network layers**, you can use\n",
    "   - **Conv layers**: decide on kernel sizes and other paramters\n",
    "   - **ReLUs** activation.\n",
    "   - **MaxPools** along width only reduce length.\n",
    "   - **Flatten** → Fully connected → ReLU → Output linear layer (1 value which should represent binding probabilty).\n",
    "\n",
    "3. **Do NOT add a final sigmoid layer** inside the model.  \n",
    "   - During training you will use `BCEWithLogitsLoss`, which internally applies a numerically stable sigmoid.  \n",
    "   - During evaluation or for thresholding, wrap outputs with `torch.sigmoid(y_hat)`.\n",
    "   - The final layer should be `nn.Linear(..., 1)`, change ... according to your architechture.\n",
    "\n",
    "If you encounter issues **Verify shapes**:\n",
    "   - After implementing, create a fake batch:\n",
    "     ```python\n",
    "     x = torch.randn(4, 1, 4, L)  # L = your sequence length\n",
    "     out = model(x)\n",
    "     print(out.shape)  # should be torch.Size([4])\n",
    "     ```\n",
    "   - If this fails, print intermediate shapes by temporarily breaking up `self.net` or inserting debug `forward` code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bee346b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, numpy as np\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "class GlobalCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    GlobalCNN\n",
    "    =========\n",
    "    A compact convolutional neural network for RNA (or DNA) binary\n",
    "    binding-site classification. It treats each sequence as a single\n",
    "    \"image\" with 1 channel, height 4 (nucleotide channels A,C,G,U),\n",
    "    and width L (sequence length), and outputs a single value per\n",
    "    sequence (before sigmoid).\n",
    "\n",
    "    Expected Input\n",
    "    --------------\n",
    "    x : array of shape (N, 1, 4, L)\n",
    "        - N = batch size\n",
    "        - 1 = dummy channel dimension (because we already use the 4 rows\n",
    "        - 4 = nucleotide axis (order must match your one-hot: A,C,G,U)\n",
    "        - L = fixed padded sequence length (e.g. 501)\n",
    "\n",
    "   \n",
    "    Output\n",
    "    ------\n",
    "    logit : array of shape (N,)\n",
    "        Raw (unnormalized) scores. Use torch.sigmoid() to\n",
    "        convert to probabilities in (0,1). Use BCEWithLogitsLoss\n",
    "        during training (so keep the final layer linear).\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    * No padding is used: width shrinks at each conv/pool step.\n",
    "    * Height is collapsed to 1 after the first 4×10 convolution,\n",
    "      effectively treating filters as motif detectors spanning all\n",
    "      nucleotide rows simultaneously.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            # TODO-1: Network architecture\n",
    "            # === Your code here ===\n",
    "\n",
    "            nn.Linear(..., 1)  # logit\n",
    "            # === End of your code ===\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : FloatTensor (N,1,4,L)\n",
    "            One-hot encoded batch.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        logits : FloatTensor (N,)\n",
    "            Raw scores (before sigmoid).\n",
    "        \"\"\"\n",
    "        return self.net(x).squeeze(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0030494f",
   "metadata": {},
   "source": [
    "## Training\n",
    "Now, we define a function to train the model.\n",
    "Pay attention that the model outputs a number $z\\in \\mathbb{R}^n$, but, we want a probability. To do so, we use the sigmoid function which converts any real number to a value between 0 and 1.\n",
    "\n",
    "Here, the skeleton code is provided as an example and you are free to change it according to any plots or information you want to add to your report.\n",
    "\n",
    "Notes:\n",
    "\n",
    "* Place the dataset files in the same directory as this notebook\n",
    "* For validation, use `roc_auc_score` from `sklearn.metrics`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e534320",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.preprocess_fa import load_dataset, MAX_LEN, DEVICE\n",
    "\n",
    "def train(model, train_loader, val_loader, epochs=30, lr=1e-3, wd=1e-4):\n",
    "    model.to(DEVICE)\n",
    "    opt  = optim.Adam(model.parameters(), lr=lr, weight_decay=wd)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    for ep in range(1, epochs+1):\n",
    "        # --- train ---\n",
    "        model.train()\n",
    "        for X,y in train_loader:\n",
    "            X,y = X.to(DEVICE), y.to(DEVICE)\n",
    "            opt.zero_grad()\n",
    "            y_hat = model(X)\n",
    "            loss = criterion(y_hat, y)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "        # --- validate ---\n",
    "        \n",
    "        # TODO-2: Evaluate the model using the validation set\n",
    "        model.eval()\n",
    "        y_hat, labels = [], []\n",
    "        with torch.no_grad():\n",
    "            for X,y in val_loader: \n",
    "                probs = torch.sigmoid(model(X.to(DEVICE))).cpu().numpy()\n",
    "                # === Your code here ===\n",
    "\n",
    "        # === End of your code ===\n",
    "\n",
    "\n",
    "# Load data and convert to 1-hot encoded matrices\n",
    "train_pos = \"./CLIPSEQ_AGO2.train.positives.fa\"\n",
    "train_neg = \"./CLIPSEQ_AGO2.train.negatives.fa\"\n",
    "val_pos   = \"./CLIPSEQ_AGO2.ls.positives.fa\"\n",
    "val_neg   = \"./CLIPSEQ_AGO2.ls.negatives.fa\"\n",
    "\n",
    "X_train, y_train = load_dataset(train_pos, train_neg)\n",
    "X_validate, y_validate = load_dataset(val_pos,   val_neg)\n",
    "\n",
    "train_ds = TensorDataset(X_train, y_train)\n",
    "val_ds   = TensorDataset(X_validate, y_validate)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=256, shuffle=True,  drop_last=True)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=256, shuffle=False)\n",
    "\n",
    "model = GlobalCNN()\n",
    "\n",
    "# Train model\n",
    "train(model, train_loader, val_loader, epochs=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6446d4f8",
   "metadata": {},
   "source": [
    "# Motif Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c56f22bb",
   "metadata": {},
   "source": [
    "In this step, we generate a large set of random synthetic sequences to serve as a baseline for model scoring. Using the provided `get_random_seqs function`, we create both the raw sequences and their one-hot encoded representations (`one_hot`). We then evaluate these sequences in batches with our trained model, collecting their binding **probabilties**\n",
    "\n",
    "Complete the code below, save in a numpy array called `scores` a list of scores for each random sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1073da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, numpy as np\n",
    "\n",
    "import importlib, utils.random_seqs\n",
    "importlib.reload(utils.random_seqs)\n",
    "from utils.random_seqs import get_random_seqs\n",
    "\n",
    "# Generate random sequences\n",
    "N_RANDOM = 100000  # how many random synthetic sequences to sample\n",
    "synthetic_seqs, one_hot = get_random_seqs(N_RANDOM)\n",
    "model.eval()\n",
    "\n",
    "# Score with model\n",
    "scores = []\n",
    "with torch.no_grad():\n",
    "    # TODO-3 : Score the random sequences in batches\n",
    "    # === Your code here ===\n",
    "    ...\n",
    "    # === End of your code ===\n",
    "\n",
    "\n",
    "assert (scores.shape[0] if hasattr(scores, \"shape\") else len(scores)) == N_RANDOM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2b68c8",
   "metadata": {},
   "source": [
    "## Save high scoring sequences to file\n",
    "After scoring the random sequences, we filter for those with predicted binding scores above a defined threshold (`BINDING_SCORE`). This identifies sequences the model considers strong binders. From each high-scoring sequence, we extract a fixed-length central window (`WINDOW_LEN`) under the arbitrary assumption that this is the relevant region for motif discovery. These positive windows are saved to a text file, which can later be uploaded to tools such as [WebLogo](https://weblogo.berkeley.edu/logo.cgi) to visualize conserved sequence motifs.\n",
    "\n",
    "**Note**: You might need to adjust `binding_score`, which is the binding probability threshold which is used to decide wether a specific sequence will be included in the PWM. Decide on a value that will only select the top $10-50$ sequences with highest binding probabiltiy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14cad54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "BINDING_SCORE    = 0.9\n",
    "WINDOW_LEN       = 100 # central window length to export\n",
    "scores = np.asarray(scores) if isinstance(scores, list) else scores\n",
    "\n",
    "# Extract positive windows\n",
    "start = (MAX_LEN - WINDOW_LEN) // 2\n",
    "end   = start + WINDOW_LEN\n",
    "pos_idx = np.where(scores >= BINDING_SCORE)[0]\n",
    "pos_windows = [synthetic_seqs[i][start:end] for i in pos_idx]\n",
    "print(f\"Positive threshold: {BINDING_SCORE:.4f}  (selected {len(pos_idx)})\")\n",
    "\n",
    "# Save\n",
    "pos_file = f\"positive.txt\"\n",
    "with open(pos_file, \"w\") as f:\n",
    "    for w in pos_windows:\n",
    "        f.write(w + \"\\n\")\n",
    "\n",
    "print(f\"Saved {len(pos_windows)} positive windows to: {pos_file}\")\n",
    "print(\"Example positive windows (first 5):\", pos_windows[:5])\n",
    "print(\"Done. The files to WebLogo to visualize the motif.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hw1-00660121",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
